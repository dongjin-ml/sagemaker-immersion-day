{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "684a86f0-4a43-4e87-9c49-30f6f1349981",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <B> Anormaly Detection based on AutoEncoder </B>\n",
    "* Container: codna_pytorch_p39"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd77507-9346-4c26-a673-754a04757384",
   "metadata": {},
   "source": [
    "## AutoEncoder based anomaly detection\n",
    "\n",
    "- **RaPP** - Novelty Detection with Reconstruction along Projection Pathway <br>\n",
    "<p align=\"center\">\n",
    "    <img src=\"imgs/rapp-f1.png\" width=\"1100\" height=\"300\" style=\"display: block; margin: 0 auto\"/>\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "    <img src=\"imgs/rapp-f2.png\" width=\"1100\" height=\"300\" style=\"display: block; margin: 0 auto\"/>\n",
    "</p>\n",
    "\n",
    "    * [Ppaer, ICLR 2020] https://openreview.net/attachment?id=HkgeGeBYDB&name=original_pdf\n",
    "    * [Desc, KOREAN] [RaPP](https://makinarocks.github.io/rapp/)\n",
    "    * [Supplement #1] [Autoencoder based Anomaly Detection](https://makinarocks.github.io/Autoencoder-based-anomaly-detection/)\n",
    "    * [Supplement #2] [Reference code (github)](https://github.com/Aiden-Jeon/RaPP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f26307-98ec-41ea-8148-9e193a470bc5",
   "metadata": {},
   "source": [
    "## AutoReload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f561b1c-8431-45c4-bcb0-ea17c1d4be17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63d1c80-b727-4641-a817-5e488cb7b638",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95dc633-3beb-4b1d-ac28-e716c045bc23",
   "metadata": {},
   "source": [
    "## parameter store 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1886719-0ce2-45b6-9de6-64bee85a645f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from utils.ssm import parameter_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2d7208-bbb5-45c3-baa7-7c8d6997a4aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strRegionName=boto3.Session().region_name\n",
    "pm = parameter_store(strRegionName)\n",
    "strPrefix = pm.get_params(key=\"PREFIX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa3a430-c567-4c99-8b97-ffc8fb314677",
   "metadata": {},
   "source": [
    "## pramamters for tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6765960-1392-45a6-8282-931863e616e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strAccountId = pm.get_params(key=\"-\".join([strPrefix, \"ACCOUNT-ID\"]))\n",
    "strBucketName = pm.get_params(key=\"-\".join([strPrefix, \"BUCKET\"]))\n",
    "strExecutionRole = pm.get_params(key=\"-\".join([strPrefix, \"SAGEMAKER-ROLE-ARN\"]))\n",
    "strS3DataPath = pm.get_params(key=\"-\".join([strPrefix, \"S3-DATA-PATH\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f444b11-5c4c-4b8d-8a88-cfec56eb291f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (f\"prefix: {strPrefix}\")\n",
    "print (f\"account_id: {strAccountId}\")\n",
    "print (f\"defaulut_bucket: {strBucketName}\")\n",
    "print (f\"sagemaker_role: {strExecutionRole}\")\n",
    "print (f\"s3_data_path: {strS3DataPath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c593b9-f579-478c-b79f-f4d39f4d4c29",
   "metadata": {},
   "source": [
    "## 1. Data manipulation and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d23e235-de3c-4a09-9efe-3b55af6f6197",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from task_utils.util import plot_click_w_fault_and_res, plot_click_w_fault_res_ad, plot_click_w_ad_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4bd696-1408-4b73-952d-0d697d13030b",
   "metadata": {},
   "source": [
    "* load data and derive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de9a8c-39f6-4664-af82-df11e36801f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clicks_1T = pd.read_csv(os.path.join(strS3DataPath, \"clicks_1T.csv\"), parse_dates=[\"timestamp\"]).set_index(\"timestamp\")\n",
    "clicks_1T[\"residual\"] = clicks_1T['click'] - clicks_1T['user'] \n",
    "clicks_1T[\"fault\"] = pd.read_csv(os.path.join(strS3DataPath, \"fault_label_1T.csv\"), header=None).values[0] ## label\n",
    "clicks_1T[\"time\"] = [int(str(time).split(\" \")[1].split(\":\")[0]) for time in clicks_1T.index] ## time variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667f927c-2d0f-4398-a582-2b183fb5f716",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (f'data shape: {clicks_1T.shape}')\n",
    "print (f'timestamp min: {clicks_1T.index.min()}, max: {clicks_1T.index.max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9155aa48-c017-400b-8278-879a7c4b1307",
   "metadata": {},
   "source": [
    "* visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ddc9e6-41ce-41ef-97bc-a5dbfcd374aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_click_w_fault_and_res(clicks_1T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b74f55-7b6e-4844-adf2-751bf074348f",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eb9de6-e1ab-4117-a19e-69f113a5a306",
   "metadata": {
    "tags": []
   },
   "source": [
    "* data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66eb102-9414-41c1-8dfe-5dd6ca7e8171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdData = clicks_1T[[\"time\", \"page\", \"user\", \"click\", \"residual\", \"fault\"]]\n",
    "print (f'Data shape: {pdData.shape}')\n",
    "pdData.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9cf998-3eee-4df9-80a4-ecb8b0b2c892",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "npData_x, npData_y = pdData[[strCol for strCol in pdData.columns if strCol != \"fault\"]].values, pdData[\"fault\"].values.reshape(-1, 1)\n",
    "npData_x, npData_time = npData_x[:, 1:], npData_x[:, 0].reshape(-1, 1)\n",
    "\n",
    "npData_x.shape, npData_y.shape, npData_time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e17ca63-9cd8-4bb0-90cd-38f62533c314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "npData_x, npData_y, npData_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787fa659-cb8a-4d56-86de-23bf9fbfd6f2",
   "metadata": {},
   "source": [
    "* StandardScaler\n",
    "    * Standardize features by removing the mean and scaling to unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228194af-6b75-4a81-85f4-b37dd344d8b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5159b4f-5957-4263-8f00-34da592a2cf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90081900-c0a9-4a87-a9ca-72b84fe6f070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler.fit(npData_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a20c3d-a557-4c08-8dba-5cb3ad457172",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "npDataScaled_x = scaler.transform(npData_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87bb1f7-4161-4a93-99e6-55d9f9ccdc11",
   "metadata": {},
   "source": [
    "* dump scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7a19f-f982-4b52-8aad-bc7a08e2ce45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(file=\"./dump/scaler.pkl\", mode=\"wb\") as f: pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ddc075-2459-48c2-9dc5-30f662c2769f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#with open(file=\"./dump/scaler.pkl\", mode=\"rb\") as f: scaler=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4f75ea-da6d-4c3a-a545-5ba8fc15fa7d",
   "metadata": {},
   "source": [
    "* shingle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31f5fc9-f44a-40b2-8b53-7c4129d0e54c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f12955-1d05-4327-bf83-d5d08a8dfd4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def shingle(data, shingle_size=32):\n",
    "    num_data, num_features = data.shape[0], data.shape[1]\n",
    "    shingled_data = np.zeros((num_data-shingle_size+1, shingle_size*num_features))\n",
    "    \n",
    "    print (num_data, shingled_data.shape)\n",
    "    \n",
    "    for idx_feature in range(num_features):\n",
    "        \n",
    "        if idx_feature == 0:\n",
    "            start, end = 0, shingle_size\n",
    "        else:\n",
    "            start = end\n",
    "            end = start + shingle_size\n",
    "                \n",
    "        for n in range(num_data - shingle_size + 1):\n",
    "            if n+shingle_size == num_data: shingled_data[n, start:end] = data[n:, idx_feature]    \n",
    "            else: shingled_data[n, start:end] = data[n:(n+shingle_size), idx_feature]\n",
    "                \n",
    "    \n",
    "    return shingled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ae331a-798d-4c60-8c52-9cf1fd9f261c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nShingleSize=4\n",
    "npDataShingle_x = shingle(npDataScaled_x, shingle_size=nShingleSize)\n",
    "npDataShingle_time = shingle(npData_time, shingle_size=nShingleSize)[:, -1].reshape(-1, 1)\n",
    "npDataShingle_y = shingle(npData_y, shingle_size=nShingleSize)[:, -1].reshape(-1, 1)\n",
    "npDataShingle_x = np.concatenate([npDataShingle_time, npDataShingle_x], axis=1)\n",
    "\n",
    "print (f'data_x_scaled_shingle: {npDataShingle_x.shape}')\n",
    "print (f'data_y_shingle: {npDataShingle_y.shape}')\n",
    "print (f'check label: {sum(npDataShingle_y == npData_y[nShingleSize-1:])}')\n",
    "print (f'fault cnt, data_y_shingle: {sum(npDataShingle_y)}, train_y: {sum(npData_y[nShingleSize-1:])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19137cf0-6d98-4dfe-84a2-72c9275e0432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# shingle_size=4\n",
    "# test_x_scaled_shingle = shingle(test_x_scaled, shingle_size=shingle_size)\n",
    "# test_y_shingle = shingle(test_y, shingle_size=shingle_size)[:, -1].reshape(-1, 1)\n",
    "\n",
    "# print (f'# features: {test_x_scaled.shape[1]}, shingle_size: {shingle_size}')\n",
    "# print (f'test_x_scaled_shingle: {test_x_scaled_shingle.shape}')\n",
    "# print (f'test_y_shingle: {test_y_shingle.shape}')\n",
    "# print (f'check label: {sum(test_y_shingle == test_y[shingle_size-1:])}')\n",
    "# print (f'fault cnt, train_y_shingle: {sum(test_y_shingle)}, train_y: {sum(test_y[shingle_size-1:])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34f1cc7-e763-4cec-b850-05dc856df825",
   "metadata": {},
   "source": [
    "## 3. AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062531f2-c19c-48a0-ac42-dde2da096066",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4980b8a2-143e-4881-bcb3-9820c88ef56e",
   "metadata": {},
   "source": [
    "* gpu setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43248912-9059-4606-a34d-699dad94294e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"# DEVICE {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(\"- Memory Usage:\")\n",
    "        print(f\"  Allocated: {round(torch.cuda.memory_allocated(i)/1024**3,1)} GB\")\n",
    "        print(f\"  Cached:    {round(torch.cuda.memory_reserved(i)/1024**3,1)} GB\\n\")\n",
    "        \n",
    "else:\n",
    "    print(\"# GPU is not available\")\n",
    "    \n",
    "# GPU 할당 변경하기\n",
    "GPU_NUM = 0 # 원하는 GPU 번호 입력\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "\n",
    "print ('# Current cuda device: ', torch.cuda.current_device()) # check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ce9a4f-b827-426c-baa2-416fa48b468f",
   "metadata": {},
   "source": [
    "* parameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21262c80-c196-4f59-bbae-e98fb2ed34e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nEpoch = 50\n",
    "nBatchSize = 128\n",
    "fLR = 1e-2\n",
    "\n",
    "nFeaures = 4 # not include time\n",
    "nShingleSize = nShingleSize\n",
    "nEmbSize = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d2443-0e65-49e0-938b-649d71cf54c3",
   "metadata": {},
   "source": [
    "* custom dataset and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495aab0e-65d5-44f6-8633-6b44f5156d1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        \n",
    "        self.x, self.y = x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        time = torch.tensor(self.x[idx, 0]).type(torch.int) \n",
    "        x = torch.tensor(self.x[idx, 1:]).type(torch.float32)\n",
    "        y = torch.tensor(self.y[idx]).type(torch.int)\n",
    "        \n",
    "        return time, x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3536333-1357-40a1-9581-39113f303b3c",
   "metadata": {},
   "source": [
    "* define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f887645a-8f0e-4eec-a39f-00dba6c3fe98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = CustomDataset(\n",
    "    x=npDataShingle_x,\n",
    "    y=npDataShingle_y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6399b8c3-0bd9-4493-a8d5-30a6e14fc044",
   "metadata": {},
   "source": [
    "* define dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d58380a-b00d-4399-966c-504c5f1ab77d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset = train_ds,\n",
    "    batch_size = nBatchSize,\n",
    "    shuffle = True,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset = train_ds,\n",
    "    batch_size = nBatchSize,\n",
    "    shuffle = False,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72f000d-c36b-482b-8302-aa1166821ca5",
   "metadata": {},
   "source": [
    "* define network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708ae788-751c-497d-84dc-2b27bc163550",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, emb_size):\n",
    "        \n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(num_embeddings=24, embedding_dim=emb_size)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def encode(self, x):\n",
    "        # |x| = (batch_size, #c, w, h)\n",
    "        z = self.encoder(x)\n",
    "        return z.view(x.size(0), -1)\n",
    "\n",
    "    def decode(self, z):\n",
    "        # |z| = (batch_size, btl_size)\n",
    "        y = self.decoder(z)\n",
    "        return y\n",
    "\n",
    "    def forward(self, time, x):\n",
    "        t_emb = self.emb(time)\n",
    "        x = torch.cat([t_emb, x], dim=1)\n",
    "        \n",
    "        z = self.encode(x)\n",
    "        x_hat = self.decode(z)\n",
    "        x_hat = x_hat\n",
    "        return t_emb, x_hat.view(x.size(0), -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fbe3c2-eeb2-460b-83d0-06c73ae5bcac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FCLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size=1, bias=True, last_act=True, bn=False, dropout_p=0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(input_size, output_size, bias)\n",
    "        self.bn = nn.BatchNorm1d(output_size) if bn else None\n",
    "        self.dropout = nn.Dropout(dropout_p) if dropout_p else None\n",
    "        if last_act: self.act = None\n",
    "        else: self.act = nn.LeakyReLU(.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "         \n",
    "        y = self.act(self.layer(x)) if self.act else self.layer(x)\n",
    "        y = self.bn(y) if self.bn else y\n",
    "        y = self.dropout(y) if self.dropout else y\n",
    "\n",
    "        return y\n",
    "\n",
    "class FCModule(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_sizes=None, use_batch_norm=True, dropout_p=0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.layer_list = []\n",
    "\n",
    "        if use_batch_norm and dropout_p > 0:\n",
    "            raise Exception(\"Either batch_norm or dropout is allowed, not both\")\n",
    "\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        for idx, (in_size, out_size) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "            if idx < len(hidden_sizes):\n",
    "                layer = FCLayer(\n",
    "                    input_size=in_size,\n",
    "                    output_size=out_size,\n",
    "                    last_act=False,\n",
    "                    bn=use_batch_norm,\n",
    "                    dropout_p=dropout_p\n",
    "                )\n",
    "            else:\n",
    "                layer = FCLayer(\n",
    "                    input_size=in_size,\n",
    "                    output_size=out_size,\n",
    "                    last_act=True,\n",
    "                )\n",
    "\n",
    "            self.layer_list.append(layer)\n",
    "        self.net = nn.Sequential(*self.layer_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2f2b8e-4366-49f7-a08a-eab06dcacb74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(input_dim, hidden_sizes, btl_size, emb_size):\n",
    "    \n",
    "    # args\n",
    "    input_size = input_dim\n",
    "    \n",
    "    encoder = FCModule(\n",
    "        input_size=input_size,\n",
    "        output_size=btl_size,\n",
    "        hidden_sizes=hidden_sizes, \n",
    "        use_batch_norm=True\n",
    "    )\n",
    "    \n",
    "    decoder = FCModule(\n",
    "        input_size=btl_size,\n",
    "        output_size=input_size,\n",
    "        hidden_sizes=list(reversed(hidden_sizes)),\n",
    "        use_batch_norm=True,\n",
    "    )\n",
    "\n",
    "    model = AutoEncoder(\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        emb_size=emb_size\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c162f674-0aed-406d-a3a5-f75ac1e05158",
   "metadata": {},
   "source": [
    "* define train class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f5af7c-a6c6-45c9-b379-d802a51754a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    \n",
    "    def __init__(self, model, optimizer, train_loader, val_loader, scheduler, device, epoch):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.epoch = epoch\n",
    "        \n",
    "        # Loss Function\n",
    "        self.criterion = nn.L1Loss().to(self.device)\n",
    "        self.anomaly_calculator = nn.L1Loss(reduction=\"none\").to(self.device)        \n",
    "        self.cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        \n",
    "    def fit(self, ):\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        best_score = 0\n",
    "        for epoch in range(self.epoch):\n",
    "            self.model.train()\n",
    "            train_loss = []\n",
    "            for time, x, y in self.train_loader:\n",
    "        \n",
    "                time, x = time.to(self.device), x.to(self.device)        \n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                t_emb, _x = self.model(time, x)\n",
    "                x = torch.cat([t_emb, x], dim=1)\n",
    "                \n",
    "                loss = self.criterion(x, _x)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "            \n",
    "            if epoch % 10 == 0 :\n",
    "                score = self.validation(self.model, 0.95)\n",
    "                diff = self.cos(x, _x).cpu().tolist()\n",
    "                print(f'Epoch : [{epoch}] Train loss : [{np.mean(train_loss)}], Train cos : [{np.mean(diff)}] Val cos : [{score}])')\n",
    "\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step(score)\n",
    "\n",
    "            if best_score < score:\n",
    "                best_score = score\n",
    "                torch.save(model.module.state_dict(), './best_model.pth', _use_new_zipfile_serialization=False)\n",
    "                \n",
    "        return self.model\n",
    "    \n",
    "    def validation(self, eval_model, thr):\n",
    "        \n",
    "        eval_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for time, x, y in self.val_loader:\n",
    "                time, x, y= time.to(self.device), x.to(self.device), y.to(self.device)\n",
    "                t_emb, _x = self.model(time, x)\n",
    "                x = torch.cat([t_emb, x], dim=1)\n",
    "                \n",
    "                anomal_score = self.anomaly_calculator(x, _x)\n",
    "                diff = self.cos(x, _x).cpu().tolist()\n",
    "                \n",
    "        return np.mean(diff)\n",
    "        \n",
    "    def prediction(self, model, test_loader, num_features, shingle_size, feature_name, emb_size):\n",
    "        model.to(self.device)\n",
    "        model.eval()\n",
    "                \n",
    "        anomal_scores = []\n",
    "        stacked = []\n",
    "        with torch.no_grad():\n",
    "            for time, x, y in test_loader:\n",
    "                \n",
    "                time, x, y= time.to(self.device), x.to(self.device), y.to(self.device)\n",
    "                t_emb, _x = model(time, x)\n",
    "                x = torch.cat([t_emb, x], dim=1)\n",
    "\n",
    "                anomal_score = self.anomaly_calculator(x[:, emb_size:], _x[:, emb_size:]) # without time\n",
    "                anomal_score_sap = 0\n",
    "                for layer in model.module.encoder.layer_list:\n",
    "                    x, _x = layer(x), layer(_x)\n",
    "                    diffs = self.anomaly_calculator(x, _x)\n",
    "                    anomal_score_sap += (diffs).mean(dim=1)\n",
    "                \n",
    "                for record, fault, sap in zip(anomal_score.cpu().numpy(), y.cpu().numpy(), anomal_score_sap.cpu().numpy()):\n",
    "                    dicScore = {\"fault\": fault[0], \"ANOMALY_SCORE_SAP\": sap}\n",
    "                    for cnt, idx in enumerate(range(0, shingle_size*num_features, shingle_size)):\n",
    "                        start = idx\n",
    "                        end = start + shingle_size\n",
    "                        dicScore[feature_name[cnt] + \"_ATTRIBUTION_SCORE\"] = np.mean(record[start:end])\n",
    "                    \n",
    "                    total_socre = 0\n",
    "                    for k, v in dicScore.items():\n",
    "                        if k not in [\"fault\", \"ANOMALY_SCORE_SAP\"]: total_socre += v\n",
    "                    dicScore[\"ANOMALY_SCORE\"] = total_socre\n",
    "                    #dicScore[\"ANOMALY_SCORE_SAP\"] = dicScore[\"ANOMALY_SCORE_SAP\"] + total_socre\n",
    "                    anomal_scores.append(dicScore)\n",
    "                    \n",
    "        return pd.DataFrame(anomal_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480ec39d-b7bf-4fe3-8a75-b2ec04e5c328",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = nn.DataParallel(\n",
    "    get_model(\n",
    "        input_dim=nFeaures*nShingleSize + nEmbSize,\n",
    "        hidden_sizes=[64, 48],\n",
    "        btl_size=32,\n",
    "        emb_size=nEmbSize\n",
    "    )\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=model.parameters(),\n",
    "    lr=fLR\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer,\n",
    "    mode='max',\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    threshold_mode='abs',\n",
    "    min_lr=1e-8,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7649db19-fe7f-4c98-9f28-6b47d6d9037f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    epoch=nEpoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63764f61-bea2-493d-a18d-6aaeb3000a42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df0cd67-3d2b-425f-aef6-7c8974190000",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_trained = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe725046-2035-465b-bf6c-504ea352733f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_name = [\"URLS\", \"USERS\", \"CLICKS\", \"RESIDUALS\"]\n",
    "pdScores = trainer.prediction(model_trained, test_loader, nFeaures, nShingleSize, feature_name, nEmbSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31df6d80-8f93-4a3b-8fc8-93a6146383e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdScores.index = pdData.iloc[nShingleSize-1:, :].index\n",
    "pdScores = pd.concat([pdScores, pdData.iloc[nShingleSize-1:, :].drop(columns='fault')], axis=1).rename(columns={\"page\":\"url\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4536f6-ffd2-418f-8bd5-22c7e559ecd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdScores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ddc89-45e6-44ac-bc78-3d958a63631b",
   "metadata": {},
   "source": [
    "## 4. Plotting the Prediction\n",
    "\n",
    "Plot the anomalous points detected by AE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcce3912-82bc-4ffd-846f-856320aadf91",
   "metadata": {},
   "source": [
    "###  - For Reconstruction Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c025b9-6c2c-4b00-912d-a0184e1d8014",
   "metadata": {
    "tags": []
   },
   "source": [
    "* Check the distribution of our anomaly score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c05a755-fb8f-4403-a9bf-e493c8208fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d82b6f-e76b-430e-9ea2-7dc420d60ee3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threshold_re = pdScores['ANOMALY_SCORE'].mean() + 2.5*pdScores['ANOMALY_SCORE'].std()\n",
    "\n",
    "pdPlot = pd.DataFrame(pdScores['ANOMALY_SCORE'])\n",
    "pdPlot.hist(bins=50)\n",
    "plt.axvline(x=threshold_re, linestyle=':', c='r')\n",
    "plt.annotate('threshold={}'.format(round(threshold_re,2)), xy=(threshold_re,6000), color='r')\n",
    "plt.show()\n",
    "\n",
    "print('3 sigma threshoud = {}'.format(threshold_re))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1e434e-dd31-4139-a027-ab1a7c01767a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anomalous = pdScores.query(\"ANOMALY_SCORE > @threshold_re\")\n",
    "stime = \"2020-07-10 09:00:00\" # \"2012-03-13 08:00:00\"\n",
    "etime = \"2020-07-10 20:00:00\" # \"2012-03-13 20:00:00\"\n",
    "plot_click_w_fault_res_ad(pdScores, anomalous, threshold_re, start_dt=stime, end_dt=etime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7702fe-f6ce-40c1-943f-1eb3de0605e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_click_w_ad_exp(pdScores, anomalous, threshold_re, start_dt=stime, end_dt=etime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b629e88a-123e-468a-b877-e4343058ae7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdAnomal = pdScores.query(\"ANOMALY_SCORE > @threshold_re\")[[\"ANOMALY_SCORE\", \"URLS_ATTRIBUTION_SCORE\", \"USERS_ATTRIBUTION_SCORE\", \\\n",
    "                                                         \"CLICKS_ATTRIBUTION_SCORE\", \"RESIDUALS_ATTRIBUTION_SCORE\", \"ANOMALY_SCORE_SAP\"]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303d35cf-d23e-4c46-b9ca-12bb09f2c91e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,5))\n",
    "fault_types = [0, 1]\n",
    "for fault_type in fault_types:\n",
    "    \n",
    "    if fault_type == 0: label = \"Normal\"\n",
    "    else: label = \"Fault\"\n",
    "    sns.distplot(\n",
    "        pdScores[pdScores.fault==fault_type][\"ANOMALY_SCORE\"],\n",
    "        hist=True,\n",
    "        kde=True,\n",
    "        kde_kws={\"shade\":True, \"linewidth\":2},\n",
    "        label=label,\n",
    "        bins=200\n",
    "    )\n",
    "plt.legend(prop={\"size\":16}, title = \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ea6023-fbcd-4f03-8c04-f738866b83a0",
   "metadata": {},
   "source": [
    "* performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe6c022-c444-4a4f-8ac6-4b3c85614364",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "precision = pdScores.query(\"fault == 1 and ANOMALY_SCORE > @threshold_re\").shape[0] / pdScores.query(\"ANOMALY_SCORE > @threshold_re\").shape[0]\n",
    "recall = pdScores.query(\"fault == 1 and ANOMALY_SCORE > @threshold_re\").shape[0] / pdScores.query(\"fault == 1\").shape[0]\n",
    "f1_score = 2*(precision*recall)/(precision+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654f3c2d-0c6e-4770-9eb8-198b8434ee5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (f'Precision: {precision}')\n",
    "print (f'Recall: {recall}')\n",
    "print (f'f1_score: {f1_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce96a699-705e-41bd-9b2e-83bcb93ae423",
   "metadata": {},
   "source": [
    "###  - For SAP Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620e65a7-73a3-4b3b-8ac4-c725ae0b978b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threshold_sap = pdScores['ANOMALY_SCORE_SAP'].mean() + 2.5*pdScores['ANOMALY_SCORE_SAP'].std()\n",
    "\n",
    "pdPlot = pd.DataFrame(pdScores['ANOMALY_SCORE_SAP'])\n",
    "pdPlot.hist(bins=150)\n",
    "plt.axvline(x=threshold_sap, linestyle=':', c='r')\n",
    "plt.annotate('threshold={}'.format(round(threshold_sap,2)), xy=(threshold_sap,6000), color='r')\n",
    "plt.show()\n",
    "\n",
    "print('3 sigma threshoud = {}'.format(threshold_sap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5e0694-f988-4c5b-b815-ed4ce4373e9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anomalous = pdScores.query(\"ANOMALY_SCORE_SAP > @threshold_sap\")\n",
    "stime = \"2020-07-10 09:00:00\" # \"2012-03-13 08:00:00\"\n",
    "etime = \"2020-07-10 20:00:00\" # \"2012-03-13 20:00:00\"\n",
    "plot_click_w_fault_res_ad(pdScores, anomalous, threshold_sap, start_dt=stime, end_dt=etime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96acc9e-b244-4dc1-9219-036679e36edb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_click_w_ad_exp(pdScores, anomalous, threshold_sap, start_dt=stime, end_dt=etime, score=\"ANOMALY_SCORE_SAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6dd11e-f35b-426e-a292-f8959d259eb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,5))\n",
    "fault_types = [0, 1]\n",
    "for fault_type in fault_types:\n",
    "    \n",
    "    if fault_type == 0: label = \"Normal\"\n",
    "    else: label = \"Fault\"\n",
    "    sns.distplot(\n",
    "        pdScores[pdScores.fault==fault_type][\"ANOMALY_SCORE_SAP\"],\n",
    "        hist=True,\n",
    "        kde=True,\n",
    "        kde_kws={\"shade\":True, \"linewidth\":3},\n",
    "        label=label,\n",
    "        bins=200\n",
    "    )\n",
    "plt.legend(prop={\"size\":16}, title = \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a51700-a34a-4702-8daf-84960b2a1efc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "precision = pdScores.query(\"fault == 1 and ANOMALY_SCORE_SAP > @threshold_sap\").shape[0] / pdScores.query(\"ANOMALY_SCORE_SAP > @threshold_sap\").shape[0]\n",
    "recall = pdScores.query(\"fault == 1 and ANOMALY_SCORE_SAP > @threshold_sap\").shape[0] / pdScores.query(\"fault == 1\").shape[0]\n",
    "f1_score = 2*(precision*recall)/(precision+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5cdfdb-7436-4640-8699-a6d4bec70282",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (f'Precision: {precision}')\n",
    "print (f'Recall: {recall}')\n",
    "print (f'f1_score: {f1_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4369bdab-67fb-4656-a805-ce03da860b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
