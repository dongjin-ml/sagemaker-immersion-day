{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "684a86f0-4a43-4e87-9c49-30f6f1349981",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <B> Anormaly Detection based on AutoEncoder </B>\n",
    "* Container: codna_pytorch_p310"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd77507-9346-4c26-a673-754a04757384",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## AutoEncoder based anomaly detection\n",
    "\n",
    "- **RaPP** - Novelty Detection with Reconstruction along Projection Pathway <br>\n",
    "<p align=\"center\">\n",
    "    <img src=\"imgs/rapp-f1.png\" width=\"1100\" height=\"300\" style=\"display: block; margin: 0 auto\"/>\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "    <img src=\"imgs/rapp-f2.png\" width=\"1100\" height=\"300\" style=\"display: block; margin: 0 auto\"/>\n",
    "</p>\n",
    "\n",
    "    * [Ppaer, ICLR 2020] https://openreview.net/attachment?id=HkgeGeBYDB&name=original_pdf\n",
    "    * [Desc, KOREAN] [RaPP](https://makinarocks.github.io/rapp/)\n",
    "    * [Supplement #1] [Autoencoder based Anomaly Detection](https://makinarocks.github.io/Autoencoder-based-anomaly-detection/)\n",
    "    * [Supplement #2] [Reference code (github)](https://github.com/Aiden-Jeon/RaPP)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f26307-98ec-41ea-8148-9e193a470bc5",
   "metadata": {},
   "source": [
    "## 0. AutoReload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f561b1c-8431-45c4-bcb0-ea17c1d4be17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b906d9cc-59b2-418d-a38c-ff3b4611416d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95dc633-3beb-4b1d-ac28-e716c045bc23",
   "metadata": {},
   "source": [
    "## 1. Parameter store 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1886719-0ce2-45b6-9de6-64bee85a645f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from utils.ssm import parameter_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2d7208-bbb5-45c3-baa7-7c8d6997a4aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strRegionName=boto3.Session().region_name\n",
    "pm = parameter_store(strRegionName)\n",
    "strPrefix = pm.get_params(key=\"PREFIX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa3a430-c567-4c99-8b97-ffc8fb314677",
   "metadata": {},
   "source": [
    "## 3. pramamters for tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6765960-1392-45a6-8282-931863e616e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strAccountId = pm.get_params(key=\"-\".join([strPrefix, \"ACCOUNT-ID\"]))\n",
    "strBucketName = pm.get_params(key=\"-\".join([strPrefix, \"BUCKET\"]))\n",
    "strExecutionRole = pm.get_params(key=\"-\".join([strPrefix, \"SAGEMAKER-ROLE-ARN\"]))\n",
    "strS3DataPath = pm.get_params(key=\"-\".join([strPrefix, \"S3-DATA-PATH\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f444b11-5c4c-4b8d-8a88-cfec56eb291f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (f\"prefix: {strPrefix}\")\n",
    "print (f\"account_id: {strAccountId}\")\n",
    "print (f\"defaulut_bucket: {strBucketName}\")\n",
    "print (f\"sagemaker_role: {strExecutionRole}\")\n",
    "print (f\"s3_data_path: {strS3DataPath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67b09d6-c2cd-4c8e-891b-79b64a5363c0",
   "metadata": {},
   "source": [
    "## 1. Data manipulation and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c5b220-1552-49e5-8401-ede8d058f181",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from task_utils.util import plot_click_w_fault_and_res, plot_click_w_fault_res_ad, plot_click_w_ad_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfd1324-6275-4988-a4c2-e97ee93ebff6",
   "metadata": {},
   "source": [
    "* load data and derive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe09a26-5485-43c3-a9c0-121231ec3fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clicks_1T = pd.read_csv(os.path.join(strS3DataPath, \"clicks_1T.csv\"), parse_dates=[\"timestamp\"]).set_index(\"timestamp\")\n",
    "clicks_1T[\"residual\"] = clicks_1T['click'] - clicks_1T['user'] \n",
    "clicks_1T[\"fault\"] = pd.read_csv(os.path.join(strS3DataPath, \"fault_label_1T.csv\"), header=None).values[0] ## label\n",
    "clicks_1T[\"time\"] = [int(str(time).split(\" \")[1].split(\":\")[0]) for time in clicks_1T.index] ## time variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd391328-02b8-452c-9612-73366ad3ae01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (f'data shape: {clicks_1T.shape}')\n",
    "print (f'timestamp min: {clicks_1T.index.min()}, max: {clicks_1T.index.max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3e019a-9a7d-447e-b6bc-552ae7f6bf46",
   "metadata": {},
   "source": [
    "* visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4261502-a0ec-4aa5-8335-9c22fd954c1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_click_w_fault_and_res(clicks_1T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a36c326-2f97-46e8-a71c-418592e2cb81",
   "metadata": {},
   "source": [
    "* upload data to s3 and local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63b3125-cce0-4c13-b9b7-1799527149cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strTrainDataName = \"merged_clicks_1T.csv\"\n",
    "clicks_1T.to_csv(os.path.join(strS3DataPath, strTrainDataName), index=True) # to s3\n",
    "clicks_1T.to_csv(os.path.join(\"./data\", strTrainDataName), index=True) # to local\n",
    "\n",
    "print (f'train_data_name: {strTrainDataName}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d61ae15-6bab-400d-9f67-fea2182c527e",
   "metadata": {},
   "source": [
    "## 2. Processing-job for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ba27a-2de5-4ab7-80b6-01ee4831d0b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.pytorch.estimator import PyTorch\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, FrameworkProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03e58b7-6bad-4ecf-92ee-2e9deae85e73",
   "metadata": {},
   "source": [
    "### Execution based on cloud / local\n",
    "* params for processing job\n",
    "    - cloud mode: `local_mode=False`\n",
    "    - local mode: `local_mode=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2a6f68-4969-4601-9327-771a710e281e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_mode = False\n",
    "\n",
    "if local_mode: \n",
    "    strInstanceType = 'local'\n",
    "    \n",
    "    import os\n",
    "    from sagemaker.local import LocalSession\n",
    "    \n",
    "    sagemaker_session = LocalSession()\n",
    "    strDataPath = str(os.path.join(\"file://\", os.getcwd(), \"data\"))\n",
    "    \n",
    "else:\n",
    "    strInstanceType = \"ml.m5.xlarge\"\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    strDataPath = strS3DataPath\n",
    "        \n",
    "print (f\"instance-type: {strInstanceType}\")\n",
    "print (f'role: {strExecutionRole}')\n",
    "print (f\"bucket: {strBucketName}\")\n",
    "print (f\"dataset-path: {strDataPath}\")\n",
    "print (f\"sagemaker_session: {sagemaker_session}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6e04e6-518f-4cf5-ab9f-77047891fd66",
   "metadata": {},
   "source": [
    "* Define processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cd8eac-9930-4901-a8b7-5c8d076ba15d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_processor = FrameworkProcessor(\n",
    "    estimator_cls=PyTorch,\n",
    "    framework_version=\"2.4.0\",\n",
    "    py_version=\"py311\",\n",
    "    image_uri=None,\n",
    "    instance_type=strInstanceType,\n",
    "    instance_count=1,\n",
    "    role=strExecutionRole,\n",
    "    base_job_name=\"preprocessing\", # bucket에 보이는 이름 (pipeline으로 묶으면 pipeline에서 정의한 이름으로 bucket에 보임)\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "strProcPrefix = \"/opt/ml/processing\"\n",
    "\n",
    "strOutputPath = os.path.join(\n",
    "    \"s3://{}\".format(strBucketName),\n",
    "    strPrefix,\n",
    "    \"preprocessing\",\n",
    "    \"output\"\n",
    ")\n",
    "\n",
    "nShingleSize = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94ee37b-56a4-4031-a914-8b77fca789fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (f'strOutputPath: {strOutputPath}')\n",
    "print (f'nShingleSize: {nShingleSize}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dab4a12-be82-41a7-8f80-31c56010c624",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_processor.run(\n",
    "    #job_name=\"preprocessing\", ## 이걸 넣어야 캐시가 작동함, 안그러면 프로세서의 base_job_name 이름뒤에 날짜 시간이 붙어서 캐시 동작 안함\n",
    "    #git_config=git_config,\n",
    "    code='preprocessing.py', #소스 디렉토리 안에서 파일 path\n",
    "    source_dir= \"./src/preprocessing\", #현재 파일에서 소스 디렉토리 상대경로 # add processing.py and requirements.txt here\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            input_name=\"input-data\",\n",
    "            source=strDataPath,\n",
    "            destination=os.path.join(strProcPrefix, \"input\")\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[       \n",
    "        ProcessingOutput(\n",
    "            output_name=\"output-data\",\n",
    "            source=os.path.join(strProcPrefix, \"output\"),\n",
    "            destination=strOutputPath\n",
    "        ),\n",
    "    ],\n",
    "    arguments=[\n",
    "        \"--proc_prefix\", strProcPrefix, \\\n",
    "        \"--shingle_size\", str(nShingleSize), \\\n",
    "        \"--train_data_name\", strTrainDataName\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59623b6-df52-4d06-ad02-fe28ea8abad7",
   "metadata": {},
   "source": [
    "* download preprocessed data to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81654bc2-14ef-4153-9361-f43151b96459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 sync $strOutputPath ./data/preprocessing --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d5f799-46ad-4190-b511-f2ed4599bfb6",
   "metadata": {},
   "source": [
    "* save params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86342d61-60aa-4b12-91b7-bb693babe417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pm.put_params(key=\"-\".join([strPrefix, \"PREP-DATA-PATH\"]), value=strOutputPath, overwrite=True)\n",
    "print (f'S3-PREP-DATA-PATH: {pm.get_params(key=\"-\".join([strPrefix, \"PREP-DATA-PATH\"]))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a90fd17-c3f1-4fe2-8b06-0b55f7b0ac8d",
   "metadata": {},
   "source": [
    "## 3. Training-job for anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1759985-42ec-4195-8fe8-15710fb4517a",
   "metadata": {},
   "source": [
    "* check gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2b4546-f1e1-48b5-aea5-6f280cf5f1be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e37cc69-a15e-4e99-ad6d-9c63c1673958",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"# DEVICE {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(\"- Memory Usage:\")\n",
    "        print(f\"  Allocated: {round(torch.cuda.memory_allocated(i)/1024**3,1)} GB\")\n",
    "        print(f\"  Cached:    {round(torch.cuda.memory_reserved(i)/1024**3,1)} GB\\n\")\n",
    "\n",
    "else:\n",
    "    print(\"# GPU is not available\")\n",
    "\n",
    "# GPU 할당 변경하기\n",
    "GPU_NUM = 0 # 원하는 GPU 번호 입력\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "\n",
    "print ('# Current cuda device: ', torch.cuda.current_device()) # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1fcc20-899c-40d9-8f94-abada550eb6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sagemaker.pytorch.estimator import PyTorch\n",
    "from sagemaker.inputs import TrainingInput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbef072-28bf-4280-891c-d1838ae11a8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "* **Set Up SageMaker Experiment**\n",
    "    - Create or load [SageMaker Experiment](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) for the example training job. This will create an experiment trial object in SageMaker.\n",
    "    - **pip instatll sagemaker-experiments**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d48fef6-34c0-4c6a-8ea4-3bfe2a547ca1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Execution based on cloud / local\n",
    "* params for processing job\n",
    "    - cloud mode: `local_mode=False`\n",
    "    - local mode: `local_mode=True`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163a80aa-8a25-49a5-809c-a8dd85000602",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Enable warmpool\n",
    "* `bUseTrainWarmPool = True`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f1a1c4-a508-429a-84f7-981c35d1e3d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "* params for training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed93fb8-0d43-4c4f-b6b9-87154ebab8a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set to True to enable SageMaker to run locally\n",
    "local_mode = True\n",
    "\n",
    "if local_mode:\n",
    "    strInstanceType = \"local_gpu\"\n",
    "    \n",
    "    import os\n",
    "    from sagemaker.local import LocalSession\n",
    "    \n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "    \n",
    "    strLocalDataDir = os.getcwd() + '/data/preprocessing'\n",
    "    dicDataChannels = {\n",
    "        \"train\": f\"file://{strLocalDataDir}\",\n",
    "        \"validation\": f\"file://{strLocalDataDir}\"\n",
    "    }\n",
    "    \n",
    "else:\n",
    "    \n",
    "    strInstanceType = \"ml.p3.2xlarge\" #\"ml.p3.2xlarge\"#\"ml.g4dn.8xlarge\"#\"ml.p3.2xlarge\", 'ml.p3.16xlarge' , ml.g4dn.8xlarge\n",
    "    \n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    dicDataChannels = {\n",
    "        \"train\": pm.get_params(key=\"-\".join([strPrefix, \"PREP-DATA-PATH\"])),\n",
    "        \"validation\": pm.get_params(key=\"-\".join([strPrefix, \"PREP-DATA-PATH\"]))\n",
    "    }\n",
    "\n",
    "dicHyperParams = {\n",
    "    \"epochs\":\"50\",\n",
    "    \"batch_size\":\"128\", \n",
    "    \"lr\":\"1e-2\",\n",
    "    \"shingle_size\":\"4\",\n",
    "    \"num_features\":\"4\",\n",
    "    \"emb_size\":\"4\",\n",
    "    \"workers\":\"2\"\n",
    "}\n",
    "\n",
    "nInstanceCount = 1\n",
    "\n",
    "bSpotTraining = False\n",
    "if bSpotTraining:\n",
    "    nMaxWait = 1*60*60\n",
    "    nMaxRun = 1*60*60\n",
    "    \n",
    "else:\n",
    "    nMaxWait = None\n",
    "    nMaxRun = 1*60*60\n",
    "\n",
    "bUseTrainWarmPool = True ## training image 다운받지 않음, 속도 빨라진다\n",
    "if bUseTrainWarmPool: nKeepAliveSeconds = 3600 ## 최대 1시간 동안!!, service quota에서 warmpool을 위한 request 필요\n",
    "else: nKeepAliveSeconds = None\n",
    "if bSpotTraining:\n",
    "    bUseTrainWarmPool = False # warmpool은 spot instance 사용시 활용 할 수 없음\n",
    "    nKeepAliveSeconds = None\n",
    "    \n",
    "    \n",
    "strProcPrefix = \"/opt/ml/processing\"\n",
    "\n",
    "strOutputPath = os.path.join(\n",
    "    \"s3://{}\".format(strBucketName),\n",
    "    strPrefix,\n",
    "    \"training\",\n",
    "    \"model-output\"\n",
    ")\n",
    "\n",
    "strCodeLocation = os.path.join(\n",
    "    \"s3://{}\".format(strBucketName),\n",
    "    strPrefix,\n",
    "    \"training\",\n",
    "    \"backup_codes\"\n",
    ")\n",
    "\n",
    "num_re = \"([0-9\\\\.]+)(e-?[[01][0-9])?\"\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"train_loss\", \"Regex\": f\"train_loss:{num_re}\"},\n",
    "    {\"Name\": \"train_cos\", \"Regex\": f\"train_cos:{num_re}\"},\n",
    "    {\"Name\": \"val_cos\", \"Regex\": f\"val_cos:{num_re}\"}\n",
    "]\n",
    "\n",
    "kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e72cd01-76cf-4a6e-a517-8429f7934966",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (f'local_mode: {local_mode}')\n",
    "print (f'sagemaker_session: {sagemaker_session}')\n",
    "print (f'strInstanceType: {strInstanceType}')\n",
    "print (f'dicDataChannels: {dicDataChannels}')\n",
    "print (f'strOutputPath: {strOutputPath}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f4ba72-53d8-4e39-b515-18b3dab5b45f",
   "metadata": {},
   "source": [
    "* Define training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae97a70-63d3-4988-8c05-069a975cc5b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    entry_point=\"main.py\", # the script we want to run\n",
    "    source_dir=\"./src/training\", # where our conf/script is\n",
    "    #git_config=git_config,\n",
    "    role=strExecutionRole,\n",
    "    instance_type=strInstanceType,\n",
    "    instance_count=nInstanceCount,\n",
    "    image_uri=None,\n",
    "    framework_version=\"2.0.0\", # version of PyTorch\n",
    "    py_version=\"py310\",\n",
    "    volume_size=128,\n",
    "    code_location=strCodeLocation,\n",
    "    output_path=strOutputPath,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    "    hyperparameters=dicHyperParams,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    metric_definitions=metric_definitions,\n",
    "    max_run=nMaxRun,\n",
    "    use_spot_instances=bSpotTraining,  # spot instance 활용\n",
    "    max_wait=nMaxWait,\n",
    "    keep_alive_period_in_seconds=nKeepAliveSeconds,\n",
    "    enable_sagemaker_metrics=True,\n",
    "    #distribution=distribution,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b34150-5dec-40e4-bd8f-20dc051dce9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if strInstanceType =='local_gpu': estimator.checkpoint_s3_uri = None\n",
    "\n",
    "#create_experiment(strExperimentName)\n",
    "#job_name = create_trial(strExperimentName)\n",
    "#job_name = \"training-ad-4\"\n",
    "estimator.fit(\n",
    "    inputs=dicDataChannels, \n",
    "    #job_name=job_name,\n",
    "    #experiment_config={\n",
    "    #  'TrialName': job_name,\n",
    "    #  'TrialComponentDisplayName': job_name,\n",
    "    #},\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17274e02-da45-465b-b14d-52015f53c8e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (f'model data: {estimator.model_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55832529-1b0f-4b47-9e5a-eaa66d2d162e",
   "metadata": {},
   "source": [
    "* save params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b21995-7369-4a1a-a4ef-a896534d04c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pm.put_params(key=\"-\".join([strPrefix, \"S3-MODEL-ARTIFACT\"]), value=estimator.model_data, overwrite=True)\n",
    "print (f'S3-MODEL-ARTIFACT: {pm.get_params(key=\"-\".join([strPrefix, \"S3-MODEL-ARTIFACT\"]))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca632e-6091-43c1-9bd2-7d585d4df5b2",
   "metadata": {},
   "source": [
    "### SageMaker Distributed Data Parallel (SMDDP)\n",
    "* code conversion to use SMDDP (`./src/trating/main_ddp.py`) \n",
    "    - warpping model with ddp\n",
    "    - local rank\n",
    "    - data loader (distributed sampler)\n",
    "* `distribution={\"smdistributed\": {\"dataparallel\": {\"enabled\": True}}}`\n",
    "\n",
    "#distribution = {\"torch_distributed\": {\"enabled\": True}} \n",
    "#distribution = {\"pytorchddp\": {\"enabled\": True}} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecd7973-e916-45e1-a615-c0b8283cc893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set to True to enable SageMaker to run locally\n",
    "local_mode = True\n",
    "\n",
    "if local_mode:\n",
    "    strInstanceType = \"local_gpu\"\n",
    "    \n",
    "    import os\n",
    "    from sagemaker.local import LocalSession\n",
    "    \n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "    \n",
    "    strLocalDataDir = os.getcwd() + '/data/preprocessing'\n",
    "    dicDataChannels = {\n",
    "        \"train\": f\"file://{strLocalDataDir}\",\n",
    "        \"validation\": f\"file://{strLocalDataDir}\"\n",
    "    }\n",
    "    \n",
    "else:\n",
    "    \n",
    "    strInstanceType = \"ml.p3.2xlarge\" #\"ml.p3.2xlarge\"#\"ml.g4dn.8xlarge\"#\"ml.p3.2xlarge\", 'ml.p3.16xlarge' , ml.g4dn.8xlarge\n",
    "    \n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    dicDataChannels = {\n",
    "        \"train\": pm.get_params(key=\"-\".join([strPrefix, \"PREP-DATA-PATH\"])),\n",
    "        \"validation\": pm.get_params(key=\"-\".join([strPrefix, \"PREP-DATA-PATH\"]))\n",
    "    }\n",
    "\n",
    "dicHyperParams = {\n",
    "    \"epochs\":\"50\",\n",
    "    \"batch_size\":\"128\", \n",
    "    \"lr\":\"1e-2\",\n",
    "    \"shingle_size\":\"4\",\n",
    "    \"num_features\":\"4\",\n",
    "    \"emb_size\":\"4\",\n",
    "    \"workers\":\"2\"\n",
    "}\n",
    "\n",
    "nInstanceCount = 1\n",
    "\n",
    "bSpotTraining = False\n",
    "if bSpotTraining:\n",
    "    nMaxWait = 1*60*60\n",
    "    nMaxRun = 1*60*60\n",
    "    \n",
    "else:\n",
    "    nMaxWait = None\n",
    "    nMaxRun = 1*60*60\n",
    "\n",
    "bUseTrainWarmPool = True ## training image 다운받지 않음, 속도 빨라진다\n",
    "if bUseTrainWarmPool: nKeepAliveSeconds = 3600 ## 최대 1시간 동안!!, service quota에서 warmpool을 위한 request 필요\n",
    "else: nKeepAliveSeconds = None\n",
    "if bSpotTraining:\n",
    "    bUseTrainWarmPool = False # warmpool은 spot instance 사용시 활용 할 수 없음\n",
    "    nKeepAliveSeconds = None\n",
    "    \n",
    "    \n",
    "strProcPrefix = \"/opt/ml/processing\"\n",
    "\n",
    "strOutputPath = os.path.join(\n",
    "    \"s3://{}\".format(strBucketName),\n",
    "    strPrefix,\n",
    "    \"training\",\n",
    "    \"model-output\"\n",
    ")\n",
    "\n",
    "strCodeLocation = os.path.join(\n",
    "    \"s3://{}\".format(strBucketName),\n",
    "    strPrefix,\n",
    "    \"training\",\n",
    "    \"backup_codes\"\n",
    ")\n",
    "\n",
    "#distribution={\"smdistributed\": {\"dataparallel\": {\"enabled\": True}}}\n",
    "distribution={\"torch_distributed\": {\"enabled\": True}}\n",
    "\n",
    "num_re = \"([0-9\\\\.]+)(e-?[[01][0-9])?\"\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"train_loss\", \"Regex\": f\"train_loss:{num_re}\"},\n",
    "    {\"Name\": \"train_cos\", \"Regex\": f\"train_cos:{num_re}\"},\n",
    "    {\"Name\": \"val_cos\", \"Regex\": f\"val_cos:{num_re}\"}\n",
    "]\n",
    "\n",
    "kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bffbfb-b47b-4116-932a-8af15e6b540b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    entry_point=\"main_ddp.py\", # the script we want to run\n",
    "    source_dir=\"./src/training\", # where our conf/script is\n",
    "    #git_config=git_config,\n",
    "    role=strExecutionRole,\n",
    "    instance_type=strInstanceType,\n",
    "    instance_count=nInstanceCount,\n",
    "    image_uri=None,\n",
    "    framework_version=\"2.0.0\", # version of PyTorch\n",
    "    py_version=\"py310\",\n",
    "    volume_size=128,\n",
    "    code_location=strCodeLocation,\n",
    "    output_path=strOutputPath,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    "    hyperparameters=dicHyperParams,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    metric_definitions=metric_definitions,\n",
    "    max_run=nMaxRun,\n",
    "    use_spot_instances=bSpotTraining,  # spot instance 활용\n",
    "    max_wait=nMaxWait,\n",
    "    keep_alive_period_in_seconds=nKeepAliveSeconds,\n",
    "    enable_sagemaker_metrics=True,\n",
    "    distribution=distribution,\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "if strInstanceType =='local_gpu':\n",
    "    estimator.checkpoint_s3_uri = None\n",
    "\n",
    "estimator.fit(\n",
    "    inputs=dicDataChannels,\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae078c4-447f-4fd2-8629-9655520ef6a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SageMaker Experiments using MLFlow\n",
    "* code conversion to use Experiments (`./src/trating/main_ddp_mlflow_exp.py`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecd1928-8cb5-403d-a70f-4d2584343174",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from time import strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa8de5c-805d-4bd5-a279-d55c8712438e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_date = strftime(\"%m%d-%H%M%s\")\n",
    "tracking_server_arn = pm.get_params(key=\"-\".join([strPrefix, \"MLFLOW-TRACKING-SERVER-ARN\"]))\n",
    "mlflow_exp_name = f'run-{strPrefix}-exp-{create_date}'\n",
    "mlflow.create_experiment(mlflow_exp_name)\n",
    "\n",
    "print (f'tracking_server_arn: {tracking_server_arn}')\n",
    "print (f'mlflow_exp_name: {mlflow_exp_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a6544a-fb99-4512-9d92-3f437a11f894",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set to True to enable SageMaker to run locally\n",
    "local_mode = False\n",
    "\n",
    "if local_mode:\n",
    "    strInstanceType = \"local_gpu\"\n",
    "    \n",
    "    import os\n",
    "    from sagemaker.local import LocalSession\n",
    "    \n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "    \n",
    "    strLocalDataDir = os.getcwd() + '/data/preprocessing'\n",
    "    dicDataChannels = {\n",
    "        \"train\": f\"file://{strLocalDataDir}\",\n",
    "        \"validation\": f\"file://{strLocalDataDir}\"\n",
    "    }\n",
    "    \n",
    "else:\n",
    "    \n",
    "    strInstanceType = \"ml.p3.2xlarge\" #\"ml.p3.2xlarge\"#\"ml.g4dn.8xlarge\"#\"ml.p3.2xlarge\", 'ml.p3.16xlarge' , ml.g4dn.8xlarge\n",
    "    \n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    dicDataChannels = {\n",
    "        \"train\": pm.get_params(key=\"-\".join([strPrefix, \"PREP-DATA-PATH\"])),\n",
    "        \"validation\": pm.get_params(key=\"-\".join([strPrefix, \"PREP-DATA-PATH\"]))\n",
    "    }\n",
    "\n",
    "dicHyperParams = {\n",
    "    \"epochs\":\"50\",\n",
    "    \"batch_size\":\"128\", \n",
    "    \"lr\":\"1e-2\",\n",
    "    \"shingle_size\":\"4\",\n",
    "    \"num_features\":\"4\",\n",
    "    \"emb_size\":\"4\",\n",
    "    \"workers\":\"2\"\n",
    "}\n",
    "\n",
    "nInstanceCount = 1\n",
    "\n",
    "bSpotTraining = False\n",
    "if bSpotTraining:\n",
    "    nMaxWait = 1*60*60\n",
    "    nMaxRun = 1*60*60\n",
    "    \n",
    "else:\n",
    "    nMaxWait = None\n",
    "    nMaxRun = 1*60*60\n",
    "\n",
    "bUseTrainWarmPool = True ## training image 다운받지 않음, 속도 빨라진다\n",
    "if bUseTrainWarmPool: nKeepAliveSeconds = 3600 ## 최대 1시간 동안!!, service quota에서 warmpool을 위한 request 필요\n",
    "else: nKeepAliveSeconds = None\n",
    "if bSpotTraining:\n",
    "    bUseTrainWarmPool = False # warmpool은 spot instance 사용시 활용 할 수 없음\n",
    "    nKeepAliveSeconds = None\n",
    "    \n",
    "    \n",
    "strProcPrefix = \"/opt/ml/processing\"\n",
    "\n",
    "strOutputPath = os.path.join(\n",
    "    \"s3://{}\".format(strBucketName),\n",
    "    strPrefix,\n",
    "    \"training\",\n",
    "    \"model-output\"\n",
    ")\n",
    "\n",
    "strCodeLocation = os.path.join(\n",
    "    \"s3://{}\".format(strBucketName),\n",
    "    strPrefix,\n",
    "    \"training\",\n",
    "    \"backup_codes\"\n",
    ")\n",
    "\n",
    "#distribution={\"smdistributed\": {\"dataparallel\": {\"enabled\": True}}}\n",
    "distribution={\"torch_distributed\": {\"enabled\": True}}\n",
    "\n",
    "environment={\n",
    "    \"MLFLOW_TRACKING_ARN\": tracking_server_arn,\n",
    "    \"EXPERIMENT_NAME\": mlflow_exp_name\n",
    "}\n",
    "strExperimentName = '-'.join([strPrefix, \"experiments-test\"])\n",
    "create_date = strftime(\"%m%d-%H%M%s\")\n",
    "strRunName = f'run-smimd-ddp-{create_date}'\n",
    "\n",
    "num_re = \"([0-9\\\\.]+)(e-?[[01][0-9])?\"\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"train_loss\", \"Regex\": f\"train_loss:{num_re}\"},\n",
    "    {\"Name\": \"train_cos\", \"Regex\": f\"train_cos:{num_re}\"},\n",
    "    {\"Name\": \"val_cos\", \"Regex\": f\"val_cos:{num_re}\"}\n",
    "]\n",
    "\n",
    "kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c39707-1a66-4fe3-9875-7364de46604d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "environment={\n",
    "    \"MLFLOW_TRACKING_ARN\": tracking_server_arn,\n",
    "    \"EXPERIMENT_NAME\": mlflow_exp_name\n",
    "}\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"main_ddp_mlflow_exp.py\", # the script we want to run\n",
    "    source_dir=\"./src/training\", # where our conf/script is\n",
    "    #git_config=git_config,\n",
    "    role=strExecutionRole,\n",
    "    instance_type=strInstanceType,\n",
    "    instance_count=nInstanceCount,\n",
    "    image_uri=None,\n",
    "    framework_version=\"2.0.0\", # version of PyTorch\n",
    "    py_version=\"py310\",\n",
    "    volume_size=128,\n",
    "    code_location=strCodeLocation,\n",
    "    output_path=strOutputPath,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    "    hyperparameters=dicHyperParams,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    metric_definitions=metric_definitions,\n",
    "    max_run=nMaxRun,\n",
    "    use_spot_instances=bSpotTraining,  # spot instance 활용\n",
    "    max_wait=nMaxWait,\n",
    "    keep_alive_period_in_seconds=nKeepAliveSeconds,\n",
    "    enable_sagemaker_metrics=True,\n",
    "    distribution=distribution,\n",
    "    environment=environment,\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "if strInstanceType =='local_gpu':\n",
    "    estimator.checkpoint_s3_uri = None\n",
    "\n",
    "estimator.fit(\n",
    "    inputs=dicDataChannels,\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85e298b-22c6-43cf-80e2-ea102003e8b2",
   "metadata": {},
   "source": [
    "### SageMaker Experiments (will be deplecated)\n",
    "* code conversion to use Experiments (`./src/trating/main_ddp_exp.py`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d752aa6e-4c0d-48e2-a04a-87703244b06e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import strftime\n",
    "from sagemaker.experiments.run import Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6eb21c-1dc7-48b1-ae2a-198d9987d30e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set to True to enable SageMaker to run locally\n",
    "local_mode = False\n",
    "\n",
    "if local_mode:\n",
    "    strInstanceType = \"local_gpu\"\n",
    "    \n",
    "    import os\n",
    "    from sagemaker.local import LocalSession\n",
    "    \n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "    \n",
    "    strLocalDataDir = os.getcwd() + '/data/preprocessing'\n",
    "    dicDataChannels = {\n",
    "        \"train\": f\"file://{strLocalDataDir}\",\n",
    "        \"validation\": f\"file://{strLocalDataDir}\"\n",
    "    }\n",
    "    \n",
    "else:\n",
    "    \n",
    "    strInstanceType = \"ml.p3.2xlarge\" #\"ml.p3.2xlarge\"#\"ml.g4dn.8xlarge\"#\"ml.p3.2xlarge\", 'ml.p3.16xlarge' , ml.g4dn.8xlarge\n",
    "    \n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    dicDataChannels = {\n",
    "        \"train\": pm.get_params(key=\"-\".join([strPrefix, \"PREP-DATA-PATH\"])),\n",
    "        \"validation\": pm.get_params(key=\"-\".join([strPrefix, \"PREP-DATA-PATH\"]))\n",
    "    }\n",
    "\n",
    "dicHyperParams = {\n",
    "    \"epochs\":\"50\",\n",
    "    \"batch_size\":\"128\", \n",
    "    \"lr\":\"1e-2\",\n",
    "    \"shingle_size\":\"4\",\n",
    "    \"num_features\":\"4\",\n",
    "    \"emb_size\":\"4\",\n",
    "    \"workers\":\"2\"\n",
    "}\n",
    "\n",
    "nInstanceCount = 1\n",
    "\n",
    "bSpotTraining = False\n",
    "if bSpotTraining:\n",
    "    nMaxWait = 1*60*60\n",
    "    nMaxRun = 1*60*60\n",
    "    \n",
    "else:\n",
    "    nMaxWait = None\n",
    "    nMaxRun = 1*60*60\n",
    "\n",
    "bUseTrainWarmPool = True ## training image 다운받지 않음, 속도 빨라진다\n",
    "if bUseTrainWarmPool: nKeepAliveSeconds = 3600 ## 최대 1시간 동안!!, service quota에서 warmpool을 위한 request 필요\n",
    "else: nKeepAliveSeconds = None\n",
    "if bSpotTraining:\n",
    "    bUseTrainWarmPool = False # warmpool은 spot instance 사용시 활용 할 수 없음\n",
    "    nKeepAliveSeconds = None\n",
    "    \n",
    "    \n",
    "strProcPrefix = \"/opt/ml/processing\"\n",
    "\n",
    "strOutputPath = os.path.join(\n",
    "    \"s3://{}\".format(strBucketName),\n",
    "    strPrefix,\n",
    "    \"training\",\n",
    "    \"model-output\"\n",
    ")\n",
    "\n",
    "strCodeLocation = os.path.join(\n",
    "    \"s3://{}\".format(strBucketName),\n",
    "    strPrefix,\n",
    "    \"training\",\n",
    "    \"backup_codes\"\n",
    ")\n",
    "\n",
    "distribution={\"smdistributed\": {\"dataparallel\": {\"enabled\": True}}}\n",
    "\n",
    "strExperimentName = '-'.join([strPrefix, \"experiments-test\"])\n",
    "create_date = strftime(\"%m%d-%H%M%s\")\n",
    "strRunName = f'run-smimd-ddp-{create_date}'\n",
    "\n",
    "num_re = \"([0-9\\\\.]+)(e-?[[01][0-9])?\"\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"train_loss\", \"Regex\": f\"train_loss:{num_re}\"},\n",
    "    {\"Name\": \"train_cos\", \"Regex\": f\"train_cos:{num_re}\"},\n",
    "    {\"Name\": \"val_cos\", \"Regex\": f\"val_cos:{num_re}\"}\n",
    "]\n",
    "\n",
    "kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2f34bd-d8e9-4221-b149-38e04508890e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with Run(\n",
    "    experiment_name=strExperimentName,\n",
    "    run_name=strRunName,\n",
    "    sagemaker_session=sagemaker_session\n",
    ") as run:\n",
    "\n",
    "    estimator = PyTorch(\n",
    "        entry_point=\"main_ddp_exp.py\", # the script we want to run\n",
    "        source_dir=\"./src/training\", # where our conf/script is\n",
    "        #git_config=git_config,\n",
    "        role=strExecutionRole,\n",
    "        instance_type=strInstanceType,\n",
    "        instance_count=nInstanceCount,\n",
    "        image_uri=None,\n",
    "        framework_version=\"2.0.0\", # version of PyTorch\n",
    "        py_version=\"py310\",\n",
    "        volume_size=128,\n",
    "        code_location=strCodeLocation,\n",
    "        output_path=strOutputPath,\n",
    "        disable_profiler=True,\n",
    "        debugger_hook_config=False,\n",
    "        hyperparameters=dicHyperParams,\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        metric_definitions=metric_definitions,\n",
    "        max_run=nMaxRun,\n",
    "        use_spot_instances=bSpotTraining,  # spot instance 활용\n",
    "        max_wait=nMaxWait,\n",
    "        keep_alive_period_in_seconds=nKeepAliveSeconds,\n",
    "        enable_sagemaker_metrics=True,\n",
    "        distribution=distribution,\n",
    "        environment=,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    if strInstanceType =='local_gpu':\n",
    "        estimator.checkpoint_s3_uri = None\n",
    "\n",
    "    estimator.fit(\n",
    "        inputs=dicDataChannels,\n",
    "        wait=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4426e9-4ccc-44d2-840e-8bd5647e13ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SageMaker HyperparameterTuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993107ad-2274-4dd1-881d-1de91dcc0cff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner, ContinuousParameter, IntegerParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175b02ff-34cd-46d6-a05a-3a97c011d8bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set to True to enable SageMaker to run locally\n",
    "local_mode = False\n",
    "\n",
    "if local_mode:\n",
    "    strInstanceType = \"local_gpu\"\n",
    "    \n",
    "    import os\n",
    "    from sagemaker.local import LocalSession\n",
    "    \n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "    \n",
    "    strLocalDataDir = os.getcwd() + '/data/preprocessing'\n",
    "    dicDataChannels = {\n",
    "        \"train\": f\"file://{strLocalDataDir}\",\n",
    "        \"validation\": f\"file://{strLocalDataDir}\"\n",
    "    }\n",
    "    \n",
    "else:\n",
    "    \n",
    "    strInstanceType = \"ml.g5.2xlarge\" #\"ml.p3.2xlarge\"#\"ml.g4dn.8xlarge\"#\"ml.p3.2xlarge\", 'ml.p3.16xlarge' , ml.g4dn.8xlarge\n",
    "    \n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    dicDataChannels = {\n",
    "        \"train\": pm.get_params(key=\"-\".join([strPrefix, \"PREP-DATA-PATH\"])),\n",
    "        \"validation\": pm.get_params(key=\"-\".join([strPrefix, \"PREP-DATA-PATH\"]))\n",
    "    }\n",
    "\n",
    "dicHyperParams = {\n",
    "    \"epochs\":\"50\",\n",
    "    \"batch_size\":\"128\", \n",
    "    \"lr\":\"1e-2\",\n",
    "    \"shingle_size\":\"4\",\n",
    "    \"num_features\":\"4\",\n",
    "    \"emb_size\":\"4\",\n",
    "    \"workers\":\"2\"\n",
    "}\n",
    "\n",
    "nInstanceCount = 1\n",
    "\n",
    "bSpotTraining = False\n",
    "if bSpotTraining:\n",
    "    nMaxWait = 1*60*60\n",
    "    nMaxRun = 1*60*60\n",
    "    \n",
    "else:\n",
    "    nMaxWait = None\n",
    "    nMaxRun = 1*60*60\n",
    "\n",
    "bUseTrainWarmPool = True ## training image 다운받지 않음, 속도 빨라진다\n",
    "if bUseTrainWarmPool: nKeepAliveSeconds = 3600 ## 최대 1시간 동안!!, service quota에서 warmpool을 위한 request 필요\n",
    "else: nKeepAliveSeconds = None\n",
    "if bSpotTraining:\n",
    "    bUseTrainWarmPool = False # warmpool은 spot instance 사용시 활용 할 수 없음\n",
    "    nKeepAliveSeconds = None\n",
    "    \n",
    "    \n",
    "strProcPrefix = \"/opt/ml/processing\"\n",
    "\n",
    "strOutputPath = os.path.join(\n",
    "    \"s3://{}\".format(strBucketName),\n",
    "    strPrefix,\n",
    "    \"training\",\n",
    "    \"model-output\"\n",
    ")\n",
    "\n",
    "strCodeLocation = os.path.join(\n",
    "    \"s3://{}\".format(strBucketName),\n",
    "    strPrefix,\n",
    "    \"training\",\n",
    "    \"backup_codes\"\n",
    ")\n",
    "\n",
    "distribution={\"smdistributed\": {\"dataparallel\": {\"enabled\": True}}}\n",
    "\n",
    "num_re = \"([0-9\\\\.]+)(e-?[[01][0-9])?\"\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"train_loss\", \"Regex\": f\"train_loss:{num_re}\"},\n",
    "    {\"Name\": \"train_cos\", \"Regex\": f\"train_cos:{num_re}\"},\n",
    "    {\"Name\": \"val_cos\", \"Regex\": f\"val_cos:{num_re}\"}\n",
    "]\n",
    "\n",
    "## hyperparam tuner\n",
    "base_tuning_job_name = '-'.join([strPrefix, \"hyperparam-test\"])\n",
    "\n",
    "#experiments\n",
    "experiment_name = '-'.join([strPrefix, \"exp-hyperparam\"])\n",
    "create_date = strftime(\"%m%d-%H%M%s\")\n",
    "strRunName = f'run-smimd-ddp-hyperparam-{create_date}'\n",
    "\n",
    "tuner_args = dict(\n",
    "    objective_metric_name=\"val_cos\",\n",
    "    objective_type=\"Maximize\", #Minimize\n",
    "    metric_definitions=metric_definitions,\n",
    "    max_jobs=4,\n",
    "    max_parallel_jobs=2,\n",
    "    early_stopping_type=\"Auto\",\n",
    ")\n",
    "\n",
    "kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2fccf9-8bbb-45ce-a779-ec66cd2aee2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    entry_point=\"main_ddp.py\", # the script we want to run\n",
    "    source_dir=\"./src/training\", # where our conf/script is\n",
    "    #git_config=git_config,\n",
    "    role=strExecutionRole,\n",
    "    instance_type=strInstanceType,\n",
    "    instance_count=nInstanceCount,\n",
    "    image_uri=None,\n",
    "    framework_version=\"2.0.0\", # version of PyTorch\n",
    "    py_version=\"py310\",\n",
    "    volume_size=128,\n",
    "    code_location=strCodeLocation,\n",
    "    output_path=strOutputPath,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    "    hyperparameters=dicHyperParams,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    metric_definitions=metric_definitions,\n",
    "    max_run=nMaxRun,\n",
    "    use_spot_instances=bSpotTraining,  # spot instance 활용\n",
    "    max_wait=nMaxWait,\n",
    "    keep_alive_period_in_seconds=nKeepAliveSeconds,\n",
    "    enable_sagemaker_metrics=True,\n",
    "    distribution=distribution,\n",
    "    environment={\"AWS_REGION\": strRegionName},\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "# Define a Hyperparameter Tuning Job\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator=estimator,\n",
    "    hyperparameter_ranges={\n",
    "        \"batch_size\": IntegerParameter(50, 100, \"Auto\"),\n",
    "    },\n",
    "    base_tuning_job_name=base_tuning_job_name,\n",
    "    **tuner_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e213d225-0091-4e64-a745-c8fab97dbd93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with Run(\n",
    "    experiment_name=experiment_name,\n",
    "    run_name=strRunName,\n",
    "    sagemaker_session=sagemaker_session\n",
    ") as run:\n",
    "\n",
    "    if strInstanceType =='local_gpu':\n",
    "        estimator.checkpoint_s3_uri = None\n",
    "\n",
    "    # Start the tuning job with the specified input data\n",
    "    tuner.fit(\n",
    "        inputs=dicDataChannels,\n",
    "        wait=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13ab5ca-0b2a-4a77-8da2-ba07796ed37d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
